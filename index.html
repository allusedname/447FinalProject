<!DOCTYPE html>
	<head>
		<title> CSE455 Final Project Demo </title>
		<link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Passion+One&display=swap" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="style.css" />
	</head>

	<body style="background-color: rgb(209, 246, 255);">
		<div id = "Header" class="round-corner">
			<h1> CSE455 Final Project</h1>
		</div>

    <div id = "Main">
      <div style="padding-top: 0px;">
        <div id = "Left" class = "section round-corner">
          <h2> Demo Video </h2>
          <video controls>
            <source src="project_video.mp4" type="video/mp4">
          </video>
        </div>
        <div id = "Member" class = "section round-corner">
          <h4>Team Member: Dongfeng Li</h4>
        </div>
      </div>

      <div id = "Right">
        <div id = "Tab" class = "section round-corner">
          <h3>Problem Setup:</h3>
          <p> This project adds an attention mechanism and pruning to the YOLOv3 model, using an open-source hand detection dataset <a href="http://www.robots.ox.ac.uk/~vgg/data/hands/">oxford hand</a> for hand detection, and model pruning based on that. 
          </p>
        </div>
        <div id = "Tab" class = "section round-corner">
          <h3>Data used:</h3>
          <p> We used pre-trained data of YOLOv3 combined with attention mechanism and pruning with the oxford hand data. 
            <br />
            <br />
            Dataset:
            <span id="bow"><a href="http://www.robots.ox.ac.uk/~vgg/data/hands/downloads/hand_dataset.tar.gz">Link</a></span>
          </p>
        </div>
        <div id = "Tab" class = "section round-corner">
          <h3>Techniques:</h3>
          <p> I trained an object detector with <a href="https://github.com/eriklindernoren/PyTorch-YOLOv3">YOLOv3</a> combined with the attention mechanism and pruning algorithm from the paper <a href="http://openaccess.thecvf.com/content_iccv_2017/html/Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.html">Learning Efficient Convolutional Networks Through Network Slimming</a>. A similar implementation can be found <a href="//github.com/talebolano/yolov3-network-slimming">here</a>.
            <br />
            The attention modules mainly include: SE, CBAM modules, information reconstruction and SEYoLo model construction for the YOLOv3 model.
            SEnet (Squeeze-and-Excitation Network) takes into account the relationship between feature channels and adds an attention mechanism to the feature channels.
            SEnet automatically learns the importance of each feature channel and uses the obtained importance to enhance the features and suppress unimportant features for the current task.
            <br />
            CBAM (Convolutional Block Attention Module) combines the attention mechanism of both feature channels and feature space dimensions. It automatically learns the importance of each feature channel, 
            similar to SENet. Additionally, it automatically learns the importance of each feature space in a similar way. It then uses the obtained importance to enhance the features and suppress unimportant features for the current task.
            CBAM extracts feature space attention in the following way: after ChannelAttention, the feature map with channel importance selected is sent to the feature space attention module. 
            Similar to the channel attention module, space attention performs max-pooling and average-pooling for each channel, and then concatenates the results. Afterward, 
            a convolution is used to reduce the feature map to a 1∗w∗h1*w*h1∗w∗h spatial weight, which is then point-multiplied with the input feature to implement the spatial attention mechanism.
            <br />
            This code is based on the paper <a href="http://openaccess.thecvf.com/content_iccv_2017/html/Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.html">Learning Efficient Convolutional Networks Through Network Slimming</a> 
            to implement an improved channel pruning algorithm. A similar implementation can be found <a href="https://github.com/talebolano/yolov3-network-slimming"> here</a>. 
            The original algorithm in the paper is for classification models and is based on pruning using the gamma coefficient of the BN layer.
            <br />
            Github repo:
            <span id="bow"><a href="https://github.com/allusedname/455FinalProject">Link</a></span>
          </p>
        </div>
        <div id = "Tab" class = "section round-corner">
          <h3>Result & Discussion:</h3>
          <p>
            Output image:
            <span id="bow"><a href="https://github.com/allusedname/455FinalProject/blob/master/imgs.zip">Link</a></span>
            <br />
            For this dataset, after channel pruning for YOLOv3, the model's parameter count and model size are reduced by 80%, FLOPs are reduced by 70%, and the forward inference speed can reach 200% of the original, while maintaining a roughly unchanged mAP.
            <table>
                <tr>
                  <td></td>
                  <td>Number of Parameters</td>
                  <td>Model Size</td>
                  <td>Flops</td>
                  <td>Forward Inference Time (2070 TI)</td>
                  <td>mAP(mean Average Precision)</td>
                </tr>
                <tr>
                  <td>Baseline (416)</td>
                  <td>61.5M</td>
                  <td>246.4MB</td>
                  <td>32.8B</td>
                  <td>15.0ms</td>
                  <td>0.6792</td>
                </tr>
                <tr>
                  <td>Prune (416)</td>
                  <td>10.9M</td>
                  <td>43.6MB</td>
                  <td>9.6B</td>
                  <td>7.7ms</td>
                  <td>0.7213</td>
                </tr>
                <tr>
                  <td>Finetune (416)</td>
                  <td>10.9M</td>
                  <td>43.6MB</td>
                  <td>9.6B</td>
                  <td>7.7ms</td>
                  <td>0.7250</td>
                </tr>
              </table>
          </p>
        </div>
      </div>

    </div>

		<div id = "Footer">

		</div>

	</body>

</html>